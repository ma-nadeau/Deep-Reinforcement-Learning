{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMP 579 - Assignment 3 \n",
    "#### Names: Marc-Antoine Nadeau (261114549) & Jessie Kurtz (26 ...)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym  # type: ignore\n",
    "import torch  # type: ignore\n",
    "import torch.nn as nn  # type: ignore\n",
    "import torch.optim as optim  # type: ignore\n",
    "import matplotlib.pyplot as plt  # type: ignore\n",
    "from tqdm import tqdm  # type: ignore\n",
    "import gymnasium as gym  # type: ignore\n",
    "import ale_py  # type: ignore\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    # Q-Network\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256, num_layers=2):\n",
    "        \"\"\"\n",
    "        Initializes the QNetwork.\n",
    "\n",
    "        Args:\n",
    "            state_dim (int): Dimension of the input state.\n",
    "            action_dim (int): Dimension of the output action.\n",
    "            hidden_dim (int, optional): Number of units in the hidden layers. Defaults to 256.\n",
    "            num_layers (int, optional): Number of hidden layers. Defaults to 2.\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer3 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "        nn.init.uniform_(self.layer1.weight, -0.001, 0.001)\n",
    "        nn.init.uniform_(self.layer2.weight, -0.001, 0.001)\n",
    "        nn.init.uniform_(self.layer3.weight, -0.001, 0.001)\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the Q-network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input state tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output Q-values tensor.\n",
    "        \"\"\"\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=1_000_000):\n",
    "        \"\"\"\n",
    "        Initializes the buffer with a specified capacity.\n",
    "\n",
    "        Args:\n",
    "            capacity (int, optional): The maximum number of elements the buffer can hold. Defaults to 1,000,000.\n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Add a new experience to the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            state (object): The current state.\n",
    "            action (object): The action taken.\n",
    "            reward (float): The reward received after taking the action.\n",
    "            next_state (object): The next state after taking the action.\n",
    "            done (bool): A flag indicating whether the episode is done.\n",
    "        \"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Samples a batch of experiences from the replay buffer.\n",
    "\n",
    "        Args:\n",
    "             batch_size (int): The number of experiences to sample.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing arrays of states, actions, rewards, next states, and done flags.\n",
    "        \"\"\"\n",
    "\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*batch)\n",
    "        return (\n",
    "            np.array(state),\n",
    "            np.array(action),\n",
    "            np.array(reward),\n",
    "            np.array(next_state),\n",
    "            np.array(done),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectedSarsaAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        lr=0.01,\n",
    "        gamma=0.99,\n",
    "        epsilon=0.1,\n",
    "        hidden_dim=256,\n",
    "        num_layers=2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the ExpectedSarsaAgent.\n",
    "\n",
    "        Args:\n",
    "            state_dim (int): Dimension of the input state.\n",
    "            action_dim (int): Dimension of the output action.\n",
    "            lr (float, optional): Learning rate. Defaults to 0.01.\n",
    "            gamma (float, optional): Discount factor. Defaults to 0.99.\n",
    "            epsilon (float, optional): Exploration rate. Defaults to 0.1.\n",
    "            hidden_dim (int, optional): Number of units in the hidden layers. Defaults to 256.\n",
    "            num_layers (int, optional): Number of hidden layers. Defaults to 2.\n",
    "        \"\"\"\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_network = QNetwork(state_dim, action_dim, hidden_dim, num_layers)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.q_network.to(self.device)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Selects an action based on the current state using an epsilon-greedy policy.\n",
    "\n",
    "        Args:\n",
    "            state (numpy.ndarray): The current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "            int: The selected action.\n",
    "        \"\"\"\n",
    "        # epsilon-greedy policy\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "                q_values = self.q_network(state)\n",
    "                return q_values.argmax().item()\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Update the Q-network based on the agent's experience.\n",
    "\n",
    "        Args:\n",
    "            state (array-like): The current state of the environment.\n",
    "            action (int): The action taken by the agent.\n",
    "            reward (float): The reward received after taking the action.\n",
    "            next_state (array-like): The state of the environment after taking the action.\n",
    "            done (bool): A flag indicating whether the episode has ended (True if done, False otherwise).\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # convert to tensors\n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        next_state = torch.FloatTensor(next_state).to(self.device)\n",
    "        action = torch.tensor(action).to(self.device)\n",
    "        reward = torch.tensor(reward).to(self.device)\n",
    "        done = torch.tensor(done, dtype=torch.float32).to(\n",
    "            self.device\n",
    "        )  # 1 if done, 0 otherwise\n",
    "\n",
    "        q_values = self.q_network(state)  # Q-values\n",
    "\n",
    "        q_value = q_values[action]  # Q-value of the action taken\n",
    "        next_q_values = self.q_network(\n",
    "            next_state\n",
    "        ).detach()  # Q-values of the next state\n",
    "\n",
    "        # E[Q(s', a')] = (1 - epsilon) * max_a' Q(s', a') + epsilon * sum_a' Q(s', a') / |A|\n",
    "        expected_q = (\n",
    "            1 - self.epsilon\n",
    "        ) * next_q_values.max() + self.epsilon * next_q_values.mean()\n",
    "        target = (\n",
    "            reward + (1 - done) * self.gamma * expected_q\n",
    "        )  # target = r + gamma * E[Q(s', a')]\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(\n",
    "            q_value, target\n",
    "        )  # = (q_value - target) ** 2\n",
    "\n",
    "        self.optimizer.zero_grad()  # reset gradients\n",
    "        loss.backward()  # backpropagation\n",
    "        self.optimizer.step()  # update weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        lr=0.01,\n",
    "        gamma=0.99,\n",
    "        epsilon=0.1,\n",
    "        hidden_dim=256,\n",
    "        num_layers=2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the QLearningAgent.\n",
    "\n",
    "        Args:\n",
    "            state_dim (int): Dimension of the input state.\n",
    "            action_dim (int): Dimension of the output action.\n",
    "            lr (float, optional): Learning rate. Defaults to 0.01.\n",
    "            gamma (float, optional): Discount factor. Defaults to 0.99.\n",
    "            epsilon (float, optional): Exploration rate. Defaults to 0.1.\n",
    "            hidden_dim (int, optional): Number of units in the hidden layers. Defaults to 256.\n",
    "            num_layers (int, optional): Number of hidden layers. Defaults to 2.\n",
    "        \"\"\"\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_network = QNetwork(state_dim, action_dim, hidden_dim, num_layers)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.q_network.to(self.device)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Selects an action based on the current state using an epsilon-greedy policy.\n",
    "\n",
    "        Args:\n",
    "            state (numpy.ndarray): The current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "            int: The selected action.\n",
    "        \"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "                q_values = self.q_network(state)\n",
    "                return torch.argmax(q_values).item()\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Update the Q-network based on the agent's experience.\n",
    "\n",
    "        Args:\n",
    "            state (array-like): The current state of the environment.\n",
    "            action (int): The action taken by the agent.\n",
    "            reward (float): The reward received after taking the action.\n",
    "            next_state (array-like): The state of the environment after taking the action.\n",
    "            done (bool): A flag indicating whether the episode has ended (True if done, False otherwise).\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # convert to tensors\n",
    "\n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        next_state = torch.FloatTensor(next_state).to(self.device)\n",
    "        action = torch.tensor(action).to(self.device)\n",
    "        reward = torch.tensor(reward).to(self.device)\n",
    "        done = torch.tensor(done, dtype=torch.float32).to(\n",
    "            self.device\n",
    "        )  # done = 1 if the episode is done, 0 otherwise\n",
    "\n",
    "        q_values = self.q_network(state)  # get the q-values\n",
    "        q_value = q_values[action]  # get the q-value for the action taken\n",
    "        next_q_values = self.q_network(\n",
    "            next_state\n",
    "        ).detach()  # get the q-values for the next state\n",
    "        max_next_q = next_q_values.max()  # get the maximum q-value\n",
    "\n",
    "        target = (\n",
    "            reward + (1 - done) * self.gamma * max_next_q\n",
    "        )  # target = reward + gamma * max_next_q\n",
    "        loss = torch.nn.functional.mse_loss(\n",
    "            q_value, target\n",
    "        )  # loss = (q_value - target)^2\n",
    "\n",
    "        # optimize the model\n",
    "        self.optimizer.zero_grad()  # set the gradients to zero\n",
    "        loss.backward()  # compute the gradients\n",
    "        self.optimizer.step()  # update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common training function for QLearning and ExpectedSarsa Agents\n",
    "def train(\n",
    "    env_name,\n",
    "    agent_class,\n",
    "    episodes=5,\n",
    "    epsilon=0.1,\n",
    "    lr=0.01,\n",
    "    trials=1,\n",
    "    use_replay=False,\n",
    "    batch_size=64,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a reinforcement learning agent in a specified environment.\n",
    "\n",
    "    Args:\n",
    "        env_name (str): The name of the environment to train in.\n",
    "        agent_class (class): The class of the agent to be trained.\n",
    "        episodes (int, optional): The number of episodes to train for each trial. Defaults to 5.\n",
    "        epsilon (float, optional): The exploration rate for the agent. Defaults to 0.1.\n",
    "        lr (float, optional): The learning rate for the agent. Defaults to 0.01.\n",
    "        trials (int, optional): The number of trials to run. Defaults to 1.\n",
    "        use_replay (bool, optional): A flag indicating whether to use a replay buffer. Defaults to False.\n",
    "    Return:\n",
    "        tuple: A tuple containing the mean and standard deviation of rewards across trials.\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    print(\n",
    "        f\"Env Made, Starting to Train: {env_name} with {'replay' if use_replay else 'no replay'} buffer, epsilon={epsilon}, lr={lr}\"\n",
    "    )\n",
    "    all_rewards = []\n",
    "    for trial in tqdm(range(trials), desc=\"Trials\", leave=False):\n",
    "        print(f\"Trial: {trial+1}\")\n",
    "        agent = agent_class(state_dim, action_dim, lr=lr, epsilon=epsilon)\n",
    "        buffer = ReplayBuffer() if use_replay else None\n",
    "        rewards = []\n",
    "        pbar = tqdm(total=episodes, desc=\"Episodes\", unit=\"episode\", leave=False)\n",
    "        for episode in range(episodes):\n",
    "            if episode % 100 == 0 and episode != 0:\n",
    "                pbar.update(100)\n",
    "            state, _ = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                if isinstance(state, tuple):\n",
    "                    state = state[0]\n",
    "                action = agent.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                total_reward += reward\n",
    "\n",
    "                if use_replay:\n",
    "                    buffer.push(state, action, reward, next_state, done)\n",
    "                    state = next_state\n",
    "\n",
    "                    # Only update if enough samples are in the buffer\n",
    "                    if len(buffer) >= batch_size:\n",
    "                        s_batch, a_batch, r_batch, ns_batch, d_batch = buffer.sample(\n",
    "                            batch_size\n",
    "                        )\n",
    "                        agent.batch_update(s_batch, a_batch, r_batch, ns_batch, d_batch)\n",
    "\n",
    "                else:\n",
    "                    agent.update(state, action, reward, next_state, done)\n",
    "                    state = next_state\n",
    "\n",
    "            rewards.append(total_reward)\n",
    "        pbar.close()\n",
    "        all_rewards.append(rewards)\n",
    "    env.close()\n",
    "    return np.mean(all_rewards, axis=0), np.std(all_rewards, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting results\n",
    "def plot_results(results_q, results_esarsa, env_name, use_replay, epsilon, lr):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Q-Learning plot\n",
    "    q_mean, q_std = results_q\n",
    "    linestyle = \"-\" if lr == 0.25 else \"--\" if lr == 0.125 else \":\"\n",
    "    plt.plot(\n",
    "        q_mean,\n",
    "        label=f\"Q-Learning ε={epsilon}, α={lr}\",\n",
    "        color=\"green\",\n",
    "        linestyle=linestyle,\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        range(len(q_mean)), q_mean - q_std, q_mean + q_std, color=\"green\", alpha=0.3\n",
    "    )\n",
    "\n",
    "    # Expected SARSA plot\n",
    "    esarsa_mean, esarsa_std = results_esarsa\n",
    "    plt.plot(\n",
    "        esarsa_mean,\n",
    "        label=f\"Expected SARSA ε={epsilon}, α={lr}\",\n",
    "        color=\"red\",\n",
    "        linestyle=linestyle,\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        range(len(esarsa_mean)),\n",
    "        esarsa_mean - esarsa_std,\n",
    "        esarsa_mean + esarsa_std,\n",
    "        color=\"red\",\n",
    "        alpha=0.3,\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(\n",
    "        f\"{env_name} {'with' if use_replay else 'without'} Replay Buffer\\nε={epsilon}, α={lr}\"\n",
    "    )\n",
    "    plt.legend()\n",
    "\n",
    "    # Ensure results directory exists\n",
    "    results_dir = \"Results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Construct unique filename dynamically\n",
    "    if env_name == \"ALE/Assault-ram-v5\":\n",
    "        env_name = \"Assault-ram-v5\"\n",
    "    filename = (\n",
    "        f\"{env_name}_{'replay' if use_replay else 'no_replay'}_eps{epsilon}_lr{lr}.png\"\n",
    "    )\n",
    "    filepath = os.path.join(results_dir, filename)\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(filepath)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epsilons for Acrobot-v1:   0%|          | 0/1 [00:00<?, ?epsilon/s]\n",
      "\u001b[A/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "Trials:   0%|          | 0/10 [01:35<?, ?it/s]\n",
      "Epsilons for Acrobot-v1:   0%|          | 0/1 [01:35<?, ?epsilon/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m agent_classes \u001b[38;5;241m=\u001b[39m [ ExpectedSarsaAgent, QLearningAgent ]\n\u001b[1;32m     46\u001b[0m use_replay_options \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m]\n\u001b[0;32m---> 47\u001b[0m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvironments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_replay_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 33\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(environments, agent_classes, use_replay_options)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epsilon \u001b[38;5;129;01min\u001b[39;00m tqdm(epsilons, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpsilons for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepsilon\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m lr \u001b[38;5;129;01min\u001b[39;00m tqdm(learning_rates,desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearning Rates for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, epsilon=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepsilon\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;66;03m# train Q-Learning and Expected SARSA agents\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m         q_mean, q_std \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_classes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m         esarsa_mean, esarsa_std \u001b[38;5;241m=\u001b[39m train(env_name, agent_classes[\u001b[38;5;241m1\u001b[39m], epsilon\u001b[38;5;241m=\u001b[39mepsilon, lr\u001b[38;5;241m=\u001b[39mlr, episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     36\u001b[0m         results_q[(epsilon, lr)] \u001b[38;5;241m=\u001b[39m (q_mean, q_std)\n",
      "Cell \u001b[0;32mIn[38], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env_name, agent_class, episodes, epsilon, lr, trials)\u001b[0m\n\u001b[1;32m     30\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[1;32m     31\u001b[0m next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 32\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     34\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[36], line 73\u001b[0m, in \u001b[0;36mExpectedSarsaAgent.update\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     70\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()(q_value, target) \u001b[38;5;66;03m# = (q_value - target) ** 2\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# reset gradients\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# backpropagation\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function for running experiments progressively\n",
    "def run_experiment(\n",
    "    environments, agent_classes, use_replay_options, epsilons, learning_rates\n",
    "):\n",
    "    for env_name in environments:\n",
    "        for use_replay in use_replay_options:\n",
    "            for epsilon in tqdm(\n",
    "                epsilons, desc=f\"Epsilons for {env_name}\", unit=\"epsilon\"\n",
    "            ):\n",
    "                for lr in tqdm(\n",
    "                    learning_rates,\n",
    "                    desc=f\"Learning Rates for {env_name}, epsilon={epsilon}\",\n",
    "                    unit=\"lr\",\n",
    "                    leave=False,\n",
    "                ):\n",
    "                    # Train Q-Learning and Expected SARSA agents\n",
    "                    print(\n",
    "                        f\"Training Q-Learning {env_name} with {'replay' if use_replay else 'no replay'} buffer, epsilon={epsilon}, lr={lr}\"\n",
    "                    )\n",
    "                    q_mean, q_std = train(\n",
    "                        env_name,\n",
    "                        agent_classes[0],\n",
    "                        epsilon=epsilon,\n",
    "                        lr=lr,\n",
    "                        episodes=1000,\n",
    "                        trials=10,\n",
    "                        use_replay=use_replay,\n",
    "                    )\n",
    "                    \n",
    "                    print(\n",
    "                        f\"Training Expected Sarsa {env_name} with {'replay' if use_replay else 'no replay'} buffer, epsilon={epsilon}, lr={lr}\"\n",
    "                    )\n",
    "                    esarsa_mean, esarsa_std = train(\n",
    "                        env_name,\n",
    "                        agent_classes[1],\n",
    "                        epsilon=epsilon,\n",
    "                        lr=lr,\n",
    "                        episodes=1000,\n",
    "                        trials=10,\n",
    "                        use_replay=use_replay,\n",
    "                    )\n",
    "\n",
    "                    # Plot and save immediately after training this configuration\n",
    "                    print(\"\\n\\n\\n plot \\n\\n\\n \")\n",
    "                    plot_results(\n",
    "                        (q_mean, q_std),\n",
    "                        (esarsa_mean, esarsa_std),\n",
    "                        env_name,\n",
    "                        use_replay,\n",
    "                        epsilon,\n",
    "                        lr,\n",
    "                    )\n",
    "\n",
    "\n",
    "# Running experiments\n",
    "if __name__ == \"__main__\":\n",
    "    learning_rates = [\n",
    "        0.01,\n",
    "        0.001,\n",
    "        0.0001,\n",
    "    ]  # https://edstem.org/us/courses/71533/discussion/6304331\n",
    "    epsilons = [0.25, 0.125, 0.0625]\n",
    "    environments = [\"Acrobot-v1\", \"ALE/Assault-ram-v5\"]\n",
    "    agent_classes = [ExpectedSarsaAgent, QLearningAgent]\n",
    "    use_replay_options = [False, True]\n",
    "    run_experiment(\n",
    "        environments, agent_classes, use_replay_options, epsilons, learning_rates\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
